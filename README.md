# Aulas-DIO
## Aprendendo e Usando o Azure Data Factory

### Como funciona o processo

- **Conectar e copiar dados:** É possivel conectar várias fontes (banco, arquivos, nuvem) e copiar os dados para um lugar só, exemplo: Azure Blob ou Data Lake.
  
![image](https://github.com/user-attachments/assets/e9dc2e1a-db2d-447f-a023-e00e9d768aee)

- **Transformar dados:** Pode-se  utilizar fluxos visuais (sem código) para limpar, juntar e modificar dados. Também pode rodar scripts em Spark, se quiser.

![image](https://github.com/user-attachments/assets/d18ff869-398a-4e68-8203-e65ef9a801ef)

- **Orquestrar etapas:** Montar o passo a passo do processo, com atividades que rodam em sequência ou paralelas, controlando tudo direitinho.
  
![image](https://github.com/user-attachments/assets/6b98edb0-770c-43d4-9ca5-f9f9bf642424)

- **Publicar e monitorar:** Depois de criar, publica e acompanha o que está rodando pelo portal, vendo erros e tempo de execução.

![image](https://github.com/user-attachments/assets/c69df44e-8fb9-4b79-9d2f-18ec4231cfe1)

### O que achei interessante

- Montar uma linha de produção dos dados, tudo bem organizado e automático.  
- Dá pra fazer transformações complexas sem precisar programar, só arrastando e configurando.  
- Integra fácil com outras ferramentas do Azure, como Synapse, e ajuda muito em projetos grandes.  
- O monitoramento é bem completo, então fica fácil saber se algo deu errado.

### Possibilidades legais

- Migrar dados de sistemas antigos para a nuvem com segurança.  
- Criar pipelines para alimentar data warehouses e relatórios.  
- Trabalhar com dados grandes e variados no Data Lake.  
- Automatizar tudo, com gatilhos e integração com Git para versionar os pipelines.
